{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Copy of simple_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5C2OTOkcObp",
        "colab_type": "text"
      },
      "source": [
        "### Introducing Keras\n",
        "\n",
        "In the next cell, we introduce [PyTorch](https://pytorch.org/), which is an open-source framework which impelments machine learning methodology, particularly that of deep neural networks, by optimizing the efficiency of the computation. We do not have to deal so much with the details of this. Most importantly, PyTorch efficiently implement backpropagation to train neural networks on the GPU.\n",
        "\n",
        "To start, we will re-implement what we did in the last section, a neural network to classify the Iris dataset, but this time we will use Keras.\n",
        "\n",
        "To start, we will re-implement what we did in the last section, a neural network to classify the Iris dataset, but this time we will use Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJCkw2K4cObr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0r8CYAPcObt",
        "colab_type": "text"
      },
      "source": [
        "Let's load the Iris dataset again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMvL3cScObu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "data, labels = iris.data[:,0:3], iris.data[:,3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kNssaZjcObw",
        "colab_type": "text"
      },
      "source": [
        "In the last lesson, we manually trained a neural network to predict the sepal width of the Iris flowers. This time, let's use the Keras library instead. First we need to shuffle and pre-process the data. Pre-processing in this case is normalization of the data, as well as converting it to a properly-shaped numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH8z7avdcObw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "77f6a92b-2493-4b45-e11e-416012051031"
      },
      "source": [
        "num_samples = len(labels)  # size of our dataset\n",
        "shuffle_order = np.random.permutation(num_samples)\n",
        "data = data[shuffle_order, :]\n",
        "labels = labels[shuffle_order]\n",
        "\n",
        "# normalize data and labels to between 0 and 1 and make sure it's float32\n",
        "data = data / np.amax(data, axis=0)\n",
        "data = data.astype('float32')\n",
        "labels = labels / np.amax(labels, axis=0)\n",
        "labels = labels.astype('float32')\n",
        "\n",
        "# print out the data\n",
        "print(\"shape of X\", data.shape)\n",
        "print(\"first 5 rows of X\\n\", data[0:5, :])\n",
        "print(\"first 5 labels\\n\", labels[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of X (150, 3)\n",
            "first 5 rows of X\n",
            " [[0.8101266  0.65909094 0.6231884 ]\n",
            " [0.84810126 0.75       0.82608694]\n",
            " [0.70886075 0.65909094 0.5217391 ]\n",
            " [0.84810126 0.6818182  0.7246377 ]\n",
            " [0.75949365 0.5        0.5797101 ]]\n",
            "first 5 labels\n",
            " [0.52 1.   0.52 0.68 0.4 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QggztlegcOby",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting and validation\n",
        "\n",
        "In our previous guides, we always evaluated the performance of the network on the same data that we trained it on. But this is wrong; our network could learn to \"cheat\" by overfitting to the training data (like memorizing it) so as to get a high score, but then not generalize well to actually unknown examples.\n",
        "\n",
        "In machine learning, this is called \"overfitting\" and there are several things we have to do to avoid it. The first thing is we must split our dataset into a \"training set\" which we train on with gradient descent, and a \"test set\" which is hidden from the training process that we can do a final evaluation on to get the true accuracy, that of the network trying to predict unknown samples.\n",
        "\n",
        "Let's split the data into a training set and a test set. We'll keep the first 30% of the dataset to use as a test set, and use the rest for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGRa_t4YcOby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80107aac-713d-4252-f611-0f9814480574"
      },
      "source": [
        "# let's rename the data and labels to X, y\n",
        "X, y = data, labels\n",
        "\n",
        "test_split = 0.3  # percent split\n",
        "\n",
        "n_test = int(test_split * num_samples)\n",
        "\n",
        "x_train, x_test = X[n_test:, :], X[:n_test, :]\n",
        "x_train = torch.from_numpy(x_train)\n",
        "x_test = torch.from_numpy(x_test)\n",
        "y_train, y_test = y[n_test:], y[:n_test] \n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "print('%d training samples, %d test samples' % (x_train.shape[0], x_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105 training samples, 45 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gICyee93dBUB",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxkm2wSrcOb1",
        "colab_type": "text"
      },
      "source": [
        "In PyTorch, to instantiate a neural network model, we inherit the class nn.Module which grants our class `Net` all the functionality of the nn.Module. We then instantiate it with the init() class. Note however, we must also instantiate the class we are inheriting from (i.e. nn.Module). This alone creates an empty neural network, so to populate it, we add the type of layer we want. In this case, we add a linear layer, a layer that is \\\"fully-connected,\\\" meaning all of its neurons are connected to all the neurons in the previous layer, with no empty connections. This may seem confusing at first because we have not yet seen neural network layers which are not fully-connected; we will see this in the next chapter when we introduce convolutional networks.\n",
        "\n",
        "Next we see the addition of the forward method. We couple the forward method with the layer(s) above to apply a variety of activation functions onto the specified layer.\n",
        "\n",
        "Finally, we will add the output layer, which will be a fully-connected layer whose size is 1 neuron. This neuron will contain our final output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAXXpKw0cOb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 8) # we get 3 from input dimension, and 8 from desired output\n",
        "        self.fc2 = nn.Linear(8, 1) # Output Layer\n",
        "    def forward(self, x):\n",
        "         x = F.sigmoid(self.fc1(x))\n",
        "         x = self.fc2(x)\n",
        "         return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7NtaUt9cOb3",
        "colab_type": "text"
      },
      "source": [
        "That may be a lot to take in, but once you fully understand the excerpt above, this structure will be used time and time again to build increasingly complex neural networks.\n",
        "\n",
        "Next we instantiate a new object based on the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHWejP99cOb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOx4Wh7zcOb5",
        "colab_type": "text"
      },
      "source": [
        "We can also get a readout of the current state of the network using `print(net)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3wQlw_UcOb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dc1af007-b33c-46dd-ce91-5145da735bc5"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=3, out_features=8, bias=True)\n",
            "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fce_uYAacOb-",
        "colab_type": "text"
      },
      "source": [
        "So we've added 9 parameters, 8x1 weights between the hidden and output layers, and 1 bias in the output. So we have 41 parameters in total."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x81TOecPcOb-",
        "colab_type": "text"
      },
      "source": [
        "Now we are finished specifying the architecture of the model. Now we need to specify our loss function and optimizer, and then compile the model. Let's discuss each of these things.\n",
        "\n",
        "First, we specify the loss. The standard for regression, as we said before is sum-squared error (SSE) or mean-squared error (MSE). SSE and MSE are basically the same, since the only difference between them is a scaling factor ($\\frac{1}{n}$) which doesn't depend on the final weights.\n",
        "\n",
        "The optimizer is the flavor of gradient descent we want. The most basic optimizer is \"stochastic gradient descent\" or SGD which is the learning algorithm we have used so far. We have mostly used batch gradient descent so far, which means we compute our gradient over the entire dataset. For reasons which will be more clear when we cover learning algorithms in more detail, this is not usually favored, and we instead calculate the gradient over random subsets of the training data, called mini-batches.\n",
        "\n",
        "Once we've specified our loss function and optimizer, the model is compiled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq7Y7KMXcOb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRvqv2MacOcA",
        "colab_type": "text"
      },
      "source": [
        "We are finally ready to train. First we must zero our gradients, so as not to rely on the previously uncovered gradient in our solution. The opposite is imperitive to RNN, as we need the last result to influence the next.\n",
        "Next we create a forward pass on our neural network. The loss function is then applied to determine the level of error. After, we complete a backwards pass to compute the new weights.  \n",
        "\n",
        "Loss is also printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr5HDpZlcOcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0baf7c9b-2de6-4e2d-f35f-1060672cdfb9"
      },
      "source": [
        "\n",
        "def fullPass(data, labels):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i in range(0,data.size()[0]):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(data[i])\n",
        "        loss = criterion(outputs, labels[i])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss\n",
        "        if i % data.size()[0] == data.size()[0]-1:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / data.size()[0]))\n",
        "            running_loss = 0.0\n",
        "\n",
        "net.train()\n",
        "for epoch in range(400):\n",
        "    fullPass(x_train, y_train);\n",
        "\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   105] loss: 0.007\n",
            "[2,   105] loss: 0.007\n",
            "[3,   105] loss: 0.007\n",
            "[4,   105] loss: 0.007\n",
            "[5,   105] loss: 0.007\n",
            "[6,   105] loss: 0.007\n",
            "[7,   105] loss: 0.007\n",
            "[8,   105] loss: 0.007\n",
            "[9,   105] loss: 0.007\n",
            "[10,   105] loss: 0.007\n",
            "[11,   105] loss: 0.007\n",
            "[12,   105] loss: 0.007\n",
            "[13,   105] loss: 0.007\n",
            "[14,   105] loss: 0.007\n",
            "[15,   105] loss: 0.007\n",
            "[16,   105] loss: 0.007\n",
            "[17,   105] loss: 0.007\n",
            "[18,   105] loss: 0.007\n",
            "[19,   105] loss: 0.007\n",
            "[20,   105] loss: 0.007\n",
            "[21,   105] loss: 0.007\n",
            "[22,   105] loss: 0.007\n",
            "[23,   105] loss: 0.007\n",
            "[24,   105] loss: 0.007\n",
            "[25,   105] loss: 0.007\n",
            "[26,   105] loss: 0.007\n",
            "[27,   105] loss: 0.007\n",
            "[28,   105] loss: 0.007\n",
            "[29,   105] loss: 0.007\n",
            "[30,   105] loss: 0.007\n",
            "[31,   105] loss: 0.007\n",
            "[32,   105] loss: 0.007\n",
            "[33,   105] loss: 0.007\n",
            "[34,   105] loss: 0.007\n",
            "[35,   105] loss: 0.007\n",
            "[36,   105] loss: 0.007\n",
            "[37,   105] loss: 0.007\n",
            "[38,   105] loss: 0.007\n",
            "[39,   105] loss: 0.007\n",
            "[40,   105] loss: 0.007\n",
            "[41,   105] loss: 0.007\n",
            "[42,   105] loss: 0.007\n",
            "[43,   105] loss: 0.007\n",
            "[44,   105] loss: 0.007\n",
            "[45,   105] loss: 0.007\n",
            "[46,   105] loss: 0.007\n",
            "[47,   105] loss: 0.007\n",
            "[48,   105] loss: 0.007\n",
            "[49,   105] loss: 0.007\n",
            "[50,   105] loss: 0.007\n",
            "[51,   105] loss: 0.007\n",
            "[52,   105] loss: 0.007\n",
            "[53,   105] loss: 0.007\n",
            "[54,   105] loss: 0.007\n",
            "[55,   105] loss: 0.007\n",
            "[56,   105] loss: 0.007\n",
            "[57,   105] loss: 0.007\n",
            "[58,   105] loss: 0.007\n",
            "[59,   105] loss: 0.007\n",
            "[60,   105] loss: 0.007\n",
            "[61,   105] loss: 0.007\n",
            "[62,   105] loss: 0.007\n",
            "[63,   105] loss: 0.007\n",
            "[64,   105] loss: 0.007\n",
            "[65,   105] loss: 0.007\n",
            "[66,   105] loss: 0.007\n",
            "[67,   105] loss: 0.007\n",
            "[68,   105] loss: 0.007\n",
            "[69,   105] loss: 0.007\n",
            "[70,   105] loss: 0.007\n",
            "[71,   105] loss: 0.007\n",
            "[72,   105] loss: 0.007\n",
            "[73,   105] loss: 0.007\n",
            "[74,   105] loss: 0.007\n",
            "[75,   105] loss: 0.007\n",
            "[76,   105] loss: 0.007\n",
            "[77,   105] loss: 0.007\n",
            "[78,   105] loss: 0.007\n",
            "[79,   105] loss: 0.007\n",
            "[80,   105] loss: 0.007\n",
            "[81,   105] loss: 0.007\n",
            "[82,   105] loss: 0.007\n",
            "[83,   105] loss: 0.007\n",
            "[84,   105] loss: 0.007\n",
            "[85,   105] loss: 0.007\n",
            "[86,   105] loss: 0.007\n",
            "[87,   105] loss: 0.007\n",
            "[88,   105] loss: 0.007\n",
            "[89,   105] loss: 0.007\n",
            "[90,   105] loss: 0.007\n",
            "[91,   105] loss: 0.007\n",
            "[92,   105] loss: 0.007\n",
            "[93,   105] loss: 0.007\n",
            "[94,   105] loss: 0.007\n",
            "[95,   105] loss: 0.007\n",
            "[96,   105] loss: 0.007\n",
            "[97,   105] loss: 0.007\n",
            "[98,   105] loss: 0.007\n",
            "[99,   105] loss: 0.007\n",
            "[100,   105] loss: 0.007\n",
            "[101,   105] loss: 0.007\n",
            "[102,   105] loss: 0.007\n",
            "[103,   105] loss: 0.007\n",
            "[104,   105] loss: 0.007\n",
            "[105,   105] loss: 0.007\n",
            "[106,   105] loss: 0.007\n",
            "[107,   105] loss: 0.007\n",
            "[108,   105] loss: 0.007\n",
            "[109,   105] loss: 0.007\n",
            "[110,   105] loss: 0.007\n",
            "[111,   105] loss: 0.007\n",
            "[112,   105] loss: 0.007\n",
            "[113,   105] loss: 0.007\n",
            "[114,   105] loss: 0.007\n",
            "[115,   105] loss: 0.007\n",
            "[116,   105] loss: 0.007\n",
            "[117,   105] loss: 0.007\n",
            "[118,   105] loss: 0.007\n",
            "[119,   105] loss: 0.007\n",
            "[120,   105] loss: 0.007\n",
            "[121,   105] loss: 0.007\n",
            "[122,   105] loss: 0.007\n",
            "[123,   105] loss: 0.007\n",
            "[124,   105] loss: 0.007\n",
            "[125,   105] loss: 0.007\n",
            "[126,   105] loss: 0.007\n",
            "[127,   105] loss: 0.007\n",
            "[128,   105] loss: 0.007\n",
            "[129,   105] loss: 0.007\n",
            "[130,   105] loss: 0.007\n",
            "[131,   105] loss: 0.007\n",
            "[132,   105] loss: 0.007\n",
            "[133,   105] loss: 0.007\n",
            "[134,   105] loss: 0.007\n",
            "[135,   105] loss: 0.007\n",
            "[136,   105] loss: 0.007\n",
            "[137,   105] loss: 0.007\n",
            "[138,   105] loss: 0.007\n",
            "[139,   105] loss: 0.007\n",
            "[140,   105] loss: 0.007\n",
            "[141,   105] loss: 0.007\n",
            "[142,   105] loss: 0.007\n",
            "[143,   105] loss: 0.007\n",
            "[144,   105] loss: 0.007\n",
            "[145,   105] loss: 0.007\n",
            "[146,   105] loss: 0.007\n",
            "[147,   105] loss: 0.007\n",
            "[148,   105] loss: 0.007\n",
            "[149,   105] loss: 0.007\n",
            "[150,   105] loss: 0.007\n",
            "[151,   105] loss: 0.007\n",
            "[152,   105] loss: 0.007\n",
            "[153,   105] loss: 0.007\n",
            "[154,   105] loss: 0.007\n",
            "[155,   105] loss: 0.007\n",
            "[156,   105] loss: 0.007\n",
            "[157,   105] loss: 0.007\n",
            "[158,   105] loss: 0.007\n",
            "[159,   105] loss: 0.007\n",
            "[160,   105] loss: 0.007\n",
            "[161,   105] loss: 0.007\n",
            "[162,   105] loss: 0.007\n",
            "[163,   105] loss: 0.007\n",
            "[164,   105] loss: 0.007\n",
            "[165,   105] loss: 0.007\n",
            "[166,   105] loss: 0.007\n",
            "[167,   105] loss: 0.007\n",
            "[168,   105] loss: 0.007\n",
            "[169,   105] loss: 0.007\n",
            "[170,   105] loss: 0.007\n",
            "[171,   105] loss: 0.007\n",
            "[172,   105] loss: 0.007\n",
            "[173,   105] loss: 0.007\n",
            "[174,   105] loss: 0.007\n",
            "[175,   105] loss: 0.007\n",
            "[176,   105] loss: 0.007\n",
            "[177,   105] loss: 0.007\n",
            "[178,   105] loss: 0.007\n",
            "[179,   105] loss: 0.007\n",
            "[180,   105] loss: 0.007\n",
            "[181,   105] loss: 0.007\n",
            "[182,   105] loss: 0.007\n",
            "[183,   105] loss: 0.007\n",
            "[184,   105] loss: 0.007\n",
            "[185,   105] loss: 0.007\n",
            "[186,   105] loss: 0.007\n",
            "[187,   105] loss: 0.007\n",
            "[188,   105] loss: 0.007\n",
            "[189,   105] loss: 0.007\n",
            "[190,   105] loss: 0.007\n",
            "[191,   105] loss: 0.007\n",
            "[192,   105] loss: 0.007\n",
            "[193,   105] loss: 0.007\n",
            "[194,   105] loss: 0.007\n",
            "[195,   105] loss: 0.007\n",
            "[196,   105] loss: 0.007\n",
            "[197,   105] loss: 0.007\n",
            "[198,   105] loss: 0.007\n",
            "[199,   105] loss: 0.007\n",
            "[200,   105] loss: 0.007\n",
            "[201,   105] loss: 0.007\n",
            "[202,   105] loss: 0.007\n",
            "[203,   105] loss: 0.007\n",
            "[204,   105] loss: 0.007\n",
            "[205,   105] loss: 0.007\n",
            "[206,   105] loss: 0.007\n",
            "[207,   105] loss: 0.007\n",
            "[208,   105] loss: 0.007\n",
            "[209,   105] loss: 0.007\n",
            "[210,   105] loss: 0.007\n",
            "[211,   105] loss: 0.007\n",
            "[212,   105] loss: 0.007\n",
            "[213,   105] loss: 0.007\n",
            "[214,   105] loss: 0.007\n",
            "[215,   105] loss: 0.007\n",
            "[216,   105] loss: 0.007\n",
            "[217,   105] loss: 0.007\n",
            "[218,   105] loss: 0.007\n",
            "[219,   105] loss: 0.007\n",
            "[220,   105] loss: 0.007\n",
            "[221,   105] loss: 0.007\n",
            "[222,   105] loss: 0.007\n",
            "[223,   105] loss: 0.007\n",
            "[224,   105] loss: 0.007\n",
            "[225,   105] loss: 0.007\n",
            "[226,   105] loss: 0.007\n",
            "[227,   105] loss: 0.007\n",
            "[228,   105] loss: 0.007\n",
            "[229,   105] loss: 0.007\n",
            "[230,   105] loss: 0.007\n",
            "[231,   105] loss: 0.007\n",
            "[232,   105] loss: 0.007\n",
            "[233,   105] loss: 0.007\n",
            "[234,   105] loss: 0.007\n",
            "[235,   105] loss: 0.007\n",
            "[236,   105] loss: 0.007\n",
            "[237,   105] loss: 0.007\n",
            "[238,   105] loss: 0.007\n",
            "[239,   105] loss: 0.007\n",
            "[240,   105] loss: 0.007\n",
            "[241,   105] loss: 0.007\n",
            "[242,   105] loss: 0.007\n",
            "[243,   105] loss: 0.007\n",
            "[244,   105] loss: 0.007\n",
            "[245,   105] loss: 0.007\n",
            "[246,   105] loss: 0.007\n",
            "[247,   105] loss: 0.007\n",
            "[248,   105] loss: 0.007\n",
            "[249,   105] loss: 0.007\n",
            "[250,   105] loss: 0.007\n",
            "[251,   105] loss: 0.007\n",
            "[252,   105] loss: 0.007\n",
            "[253,   105] loss: 0.007\n",
            "[254,   105] loss: 0.007\n",
            "[255,   105] loss: 0.007\n",
            "[256,   105] loss: 0.007\n",
            "[257,   105] loss: 0.007\n",
            "[258,   105] loss: 0.007\n",
            "[259,   105] loss: 0.007\n",
            "[260,   105] loss: 0.007\n",
            "[261,   105] loss: 0.007\n",
            "[262,   105] loss: 0.007\n",
            "[263,   105] loss: 0.007\n",
            "[264,   105] loss: 0.007\n",
            "[265,   105] loss: 0.007\n",
            "[266,   105] loss: 0.007\n",
            "[267,   105] loss: 0.007\n",
            "[268,   105] loss: 0.007\n",
            "[269,   105] loss: 0.007\n",
            "[270,   105] loss: 0.007\n",
            "[271,   105] loss: 0.007\n",
            "[272,   105] loss: 0.007\n",
            "[273,   105] loss: 0.007\n",
            "[274,   105] loss: 0.007\n",
            "[275,   105] loss: 0.007\n",
            "[276,   105] loss: 0.007\n",
            "[277,   105] loss: 0.007\n",
            "[278,   105] loss: 0.007\n",
            "[279,   105] loss: 0.007\n",
            "[280,   105] loss: 0.007\n",
            "[281,   105] loss: 0.007\n",
            "[282,   105] loss: 0.007\n",
            "[283,   105] loss: 0.007\n",
            "[284,   105] loss: 0.007\n",
            "[285,   105] loss: 0.007\n",
            "[286,   105] loss: 0.007\n",
            "[287,   105] loss: 0.007\n",
            "[288,   105] loss: 0.007\n",
            "[289,   105] loss: 0.007\n",
            "[290,   105] loss: 0.007\n",
            "[291,   105] loss: 0.007\n",
            "[292,   105] loss: 0.007\n",
            "[293,   105] loss: 0.007\n",
            "[294,   105] loss: 0.007\n",
            "[295,   105] loss: 0.007\n",
            "[296,   105] loss: 0.007\n",
            "[297,   105] loss: 0.007\n",
            "[298,   105] loss: 0.007\n",
            "[299,   105] loss: 0.007\n",
            "[300,   105] loss: 0.007\n",
            "[301,   105] loss: 0.007\n",
            "[302,   105] loss: 0.007\n",
            "[303,   105] loss: 0.007\n",
            "[304,   105] loss: 0.007\n",
            "[305,   105] loss: 0.007\n",
            "[306,   105] loss: 0.007\n",
            "[307,   105] loss: 0.007\n",
            "[308,   105] loss: 0.007\n",
            "[309,   105] loss: 0.007\n",
            "[310,   105] loss: 0.007\n",
            "[311,   105] loss: 0.007\n",
            "[312,   105] loss: 0.007\n",
            "[313,   105] loss: 0.007\n",
            "[314,   105] loss: 0.007\n",
            "[315,   105] loss: 0.007\n",
            "[316,   105] loss: 0.007\n",
            "[317,   105] loss: 0.007\n",
            "[318,   105] loss: 0.007\n",
            "[319,   105] loss: 0.007\n",
            "[320,   105] loss: 0.007\n",
            "[321,   105] loss: 0.007\n",
            "[322,   105] loss: 0.007\n",
            "[323,   105] loss: 0.007\n",
            "[324,   105] loss: 0.007\n",
            "[325,   105] loss: 0.007\n",
            "[326,   105] loss: 0.007\n",
            "[327,   105] loss: 0.007\n",
            "[328,   105] loss: 0.007\n",
            "[329,   105] loss: 0.007\n",
            "[330,   105] loss: 0.007\n",
            "[331,   105] loss: 0.007\n",
            "[332,   105] loss: 0.007\n",
            "[333,   105] loss: 0.007\n",
            "[334,   105] loss: 0.007\n",
            "[335,   105] loss: 0.007\n",
            "[336,   105] loss: 0.007\n",
            "[337,   105] loss: 0.007\n",
            "[338,   105] loss: 0.007\n",
            "[339,   105] loss: 0.007\n",
            "[340,   105] loss: 0.007\n",
            "[341,   105] loss: 0.007\n",
            "[342,   105] loss: 0.007\n",
            "[343,   105] loss: 0.007\n",
            "[344,   105] loss: 0.007\n",
            "[345,   105] loss: 0.007\n",
            "[346,   105] loss: 0.007\n",
            "[347,   105] loss: 0.007\n",
            "[348,   105] loss: 0.007\n",
            "[349,   105] loss: 0.007\n",
            "[350,   105] loss: 0.007\n",
            "[351,   105] loss: 0.007\n",
            "[352,   105] loss: 0.007\n",
            "[353,   105] loss: 0.007\n",
            "[354,   105] loss: 0.007\n",
            "[355,   105] loss: 0.007\n",
            "[356,   105] loss: 0.007\n",
            "[357,   105] loss: 0.007\n",
            "[358,   105] loss: 0.007\n",
            "[359,   105] loss: 0.007\n",
            "[360,   105] loss: 0.007\n",
            "[361,   105] loss: 0.007\n",
            "[362,   105] loss: 0.007\n",
            "[363,   105] loss: 0.007\n",
            "[364,   105] loss: 0.007\n",
            "[365,   105] loss: 0.007\n",
            "[366,   105] loss: 0.007\n",
            "[367,   105] loss: 0.007\n",
            "[368,   105] loss: 0.007\n",
            "[369,   105] loss: 0.007\n",
            "[370,   105] loss: 0.007\n",
            "[371,   105] loss: 0.007\n",
            "[372,   105] loss: 0.007\n",
            "[373,   105] loss: 0.007\n",
            "[374,   105] loss: 0.007\n",
            "[375,   105] loss: 0.007\n",
            "[376,   105] loss: 0.007\n",
            "[377,   105] loss: 0.007\n",
            "[378,   105] loss: 0.007\n",
            "[379,   105] loss: 0.007\n",
            "[380,   105] loss: 0.007\n",
            "[381,   105] loss: 0.007\n",
            "[382,   105] loss: 0.007\n",
            "[383,   105] loss: 0.007\n",
            "[384,   105] loss: 0.007\n",
            "[385,   105] loss: 0.007\n",
            "[386,   105] loss: 0.007\n",
            "[387,   105] loss: 0.007\n",
            "[388,   105] loss: 0.007\n",
            "[389,   105] loss: 0.007\n",
            "[390,   105] loss: 0.007\n",
            "[391,   105] loss: 0.007\n",
            "[392,   105] loss: 0.007\n",
            "[393,   105] loss: 0.007\n",
            "[394,   105] loss: 0.007\n",
            "[395,   105] loss: 0.007\n",
            "[396,   105] loss: 0.007\n",
            "[397,   105] loss: 0.007\n",
            "[398,   105] loss: 0.007\n",
            "[399,   105] loss: 0.007\n",
            "[400,   105] loss: 0.007\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b--6NRFcOcD",
        "colab_type": "text"
      },
      "source": [
        "As you can see above, we train our network down to a validation MSE < 0.01. Notice that both the training loss (\"loss\") and validation loss (\"val_loss\") are reported. It's normal for the training loss to be lower than the validation loss, since the network's objective is to predict the training data well. But if the training loss is much lower than our validation loss, it means we are overfitting and may not expect to receive very good results.\n",
        "\n",
        "We can evaluate the training set one last time at the end using `eval`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bBw-4ficOcD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6d0f3912-032e-4e66-ef44-cd7a949fee2f"
      },
      "source": [
        "net.eval()\n",
        "fullPass(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[400,    45] loss: 0.009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kikh9Fm8cOcH",
        "colab_type": "text"
      },
      "source": [
        "We can manually calculate MSE as a sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivo6Wcj9cOcH",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7a7dc69-a160-4c16-c253-d7ee8f9a7c36"
      },
      "source": [
        "def MSE(y_pred, y_test):\n",
        "    return (1.0/len(y_test)) * np.sum([((y1[0]-y2)**2) for y1, y2 in list(zip(y_pred, y_test))])\n",
        "\n",
        "print(\"MSE is %0.4f\" % MSE(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE is 0.0092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjaX5ZL9cOcK",
        "colab_type": "text"
      },
      "source": [
        "We can also predict the value of a single unknown example or a set of them in th following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab42VftocOcK",
        "colab_type": "code",
        "colab": {},
        "outputId": "ec88b4b3-e7f3-4068-dc03-0c05d00b9345"
      },
      "source": [
        "x_sample = x_test[0].reshape(1, 3)   # shape must be (num_samples, 3), even if num_samples = 1\n",
        "y_prob = model.predict(x_sample)\n",
        "\n",
        "print(\"predicted %0.3f, actual %0.3f\" % (y_prob[0][0], y_test[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted 0.723, actual 0.960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sBmZx0dcOcM",
        "colab_type": "text"
      },
      "source": [
        "We've now finished introducing PyTorch for regression. Note it is a far more powerful way of training neural networks than our own. PyTorch's strengths will become even more apparent when we introduce classification in the next lesson, as well as introduce convolutional networks and various other optimization tricks it enables for us."
      ]
    }
  ]
}