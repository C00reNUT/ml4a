{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of transfer-learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ef092b1d04246c996d11c13094e53ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9043f4d12dcd494785c7af625769d4d9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d061078c43b476d92e38fb778a2b5e0",
              "IPY_MODEL_0dfd6845b281411985de3fa1bd2aaa6c"
            ]
          }
        },
        "9043f4d12dcd494785c7af625769d4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d061078c43b476d92e38fb778a2b5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_25d6c29fb5e340668c76536539f48443",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b7800fe13a84b038bfaa264445cd256"
          }
        },
        "0dfd6845b281411985de3fa1bd2aaa6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a11663c69f4b461f87d062ff06dbc125",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640404480/? [02:30&lt;00:00, 22900982.94it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66c44b0e9e6c44cdbf4016b62cdffb05"
          }
        },
        "25d6c29fb5e340668c76536539f48443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b7800fe13a84b038bfaa264445cd256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a11663c69f4b461f87d062ff06dbc125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66c44b0e9e6c44cdbf4016b62cdffb05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92mQ2gMlYoZl",
        "colab_type": "text"
      },
      "source": [
        "# Transfer learning / fine-tuning\n",
        "\n",
        "This tutorial will guide you through the process of using _transfer learning_ to learn an accurate image classifier from a relatively small number of training samples. Generally speaking, transfer learning refers to the process of leveraging the knowledge learned in one model for the training of another model. \n",
        "\n",
        "More specifically, the process involves taking an existing neural network which was previously trained to good performance on a larger dataset, and using it as the basis for a new model which leverages that previous network's accuracy for a new task. This method has become popular in recent years to improve the performance of a neural net trained on a small dataset; the intuition is that the new dataset may be too small to train to good performance by itself, but we know that most neural nets trained to learn image features often learn similar features anyway, especially at early layers where they are more generic (edge detectors, blobs, and so on). \n",
        "\n",
        "Transfer learning has been largely enabled by the open-sourcing of state-of-the-art models; for the top performing models in image classification tasks (like from [ILSVRC](http://www.image-net.org/challenges/LSVRC/)), it is common practice now to not only publish the architecture, but to release the trained weights of the model as well. This lets amateurs use these top image classifiers to boost the performance of their own task-specific models.\n",
        "\n",
        "#### Feature extraction vs. fine-tuning\n",
        "\n",
        "At one extreme, transfer learning can involve taking the pre-trained network and freezing the weights, and using one of its hidden layers (usually the last one) as a feature extractor, using those features as the input to a smaller neural net. \n",
        "\n",
        "At the other extreme, we start with the pre-trained network, but we allow some of the weights (usually the last layer or last few layers) to be modified. Another name for this procedure is called \"fine-tuning\" because we are slightly adjusting the pre-trained net's weights to the new task. We usually train such a network with a lower learning rate, since we expect the features are already relatively good and do not need to be changed too much. \n",
        "\n",
        "Sometimes, we do something in-between: Freeze just the early/generic layers, but fine-tune the later layers. Which strategy is best depends on the size of your dataset, the number of classes, and how much it resembles the dataset the previous model was trained on (and thus, whether it can benefit from the same learned feature extractors). A more detailed discussion of how to strategize can be found in [[1]](http://cs231n.github.io/transfer-learning/) [[2]](http://sebastianruder.com/transfer-learning/).\n",
        "\n",
        "## Procedure\n",
        "\n",
        "In this guide will go through the process of loading a state-of-the-art, 1000-class image classifier, [VGG16](https://arxiv.org/pdf/1409.1556.pdf) which [won the ImageNet challenge in 2014](http://www.robots.ox.ac.uk/~vgg/research/very_deep/), and using it as a fixed feature extractor to train a smaller custom classifier on our own images, although with very few code changes, you can try fine-tuning as well.\n",
        "\n",
        "We will first load VGG16 and remove its final layer, the 1000-class softmax classification layer specific to ImageNet, and replace it with a new classification layer for the classes we are training over. We will then freeze all the weights in the network except the new ones connecting to the new classification layer, and then train the new classification layer over our new dataset. \n",
        "\n",
        "We will also compare this method to training a small neural network from scratch on the new dataset, and as we shall see, it will dramatically improve our accuracy. We will do that part first.\n",
        "\n",
        "As our test subject, we'll use a dataset consisting of around 6000 images belonging to 97 classes, and train an image classifier with around 80% accuracy on it. It's worth noting that this strategy scales well to image sets where you may have even just a couple hundred or less images. Its performance will be lesser from a small number of samples (depending on classes) as usual, but still impressive considering the usual constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p-OjhDPYoZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWN-FPLYoZs",
        "colab_type": "text"
      },
      "source": [
        "### Getting a dataset\n",
        "\n",
        "The first step is going to be to load our data. As our example, we will be using the dataset [STL-10](https://cs.stanford.edu/~acoates/stl10/), which contains thousands of labeled images belonging to 10 object categories. In order to handle this large sum of data, we prescribe it the dataloader from torchvision. This will grab individual pieces of the dataset used for training in batches.\n",
        "\n",
        "If wanting to use a dataset outside the scope of pytorch, feel free to still use the dataloader, but the retrieval method will vary depending on the library. A handy tool as an alternative to wget is `gdown` which seamlessly integrates with the colab environment.\n",
        "\n",
        "If you wish to use your own dataset, it should be aranged in the same fashion to `LSUN` with all of the images organized into subfolders, one for each class. In this case, the following cell should load your custom dataset correctly by just replacing `root` with your folder. If you have an alternate structure, you just need to make sure that you load the list `data` where every element is a dict where `x` is the data (a 1-d numpy array) and `y` is the label (an integer). Use the helper function `get_image(path)` to load the image correctly into the array, and note also that the images are being resized to 224x224. This is necessary because the input to VGG16 is a 224x224 RGB image. You do not need to resize them on your hard drive, as that is being done in the code below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XklKIrnaZb3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "3ef092b1d04246c996d11c13094e53ed",
            "9043f4d12dcd494785c7af625769d4d9",
            "7d061078c43b476d92e38fb778a2b5e0",
            "0dfd6845b281411985de3fa1bd2aaa6c",
            "25d6c29fb5e340668c76536539f48443",
            "0b7800fe13a84b038bfaa264445cd256",
            "a11663c69f4b461f87d062ff06dbc125",
            "66c44b0e9e6c44cdbf4016b62cdffb05"
          ]
        },
        "outputId": "c18f21de-4511-4b16-fd62-9bec8c69937c"
      },
      "source": [
        "stl_data = torchvision.datasets.STL10(\"/\", folds=None, transform=None, download=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /stl10_binary.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ef092b1d04246c996d11c13094e53ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /stl10_binary.tar.gz to /\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OGRcLNwYoZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "481bf792-96cc-4e9d-f181-6bf45bd9febe"
      },
      "source": [
        "root = 'stl10_binary'\n",
        "classes = open(\"/stl10_binary/class_names.txt\")\n",
        "print(classes.read())\n",
        "classes.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "airplane\n",
            "bird\n",
            "car\n",
            "cat\n",
            "deer\n",
            "dog\n",
            "horse\n",
            "monkey\n",
            "ship\n",
            "truck\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ERhVlFYoZy",
        "colab_type": "text"
      },
      "source": [
        "This function is useful for pre-processing the data into an image and input vector. Resize the image to the appropriate dimensions. Load all the images from root folder. Randomize the data order. Pre-process the data as before by making sure it's float32 and normalized between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1T1Joq7YoZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to load image and return it and input vector\n",
        "reshape = transforms.Compose([\n",
        "                    transforms.Resize(256), \n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxwJDLBS-Z6l",
        "colab_type": "text"
      },
      "source": [
        "Load the data and apply the transformarions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nAUr-ooYoZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stl_train = torchvision.datasets.STL10(\"/\", split='train', folds=None, transform=reshape)\n",
        "                                      \n",
        "train_loader = torch.utils.data.DataLoader(stl_train,\n",
        "                                          batch_size=12,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=0)\n",
        "stl_test = torchvision.datasets.STL10(\"/\", split='test', folds=None, transform=reshape)\n",
        "                                      \n",
        "test_loader = torch.utils.data.DataLoader(stl_test,\n",
        "                                          batch_size=12,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-P9MNPcYoaY",
        "colab_type": "text"
      },
      "source": [
        "If everything worked properly, you should have loaded a bunch of images, and split them into three sets: `train`, `val`, and `test`.\n",
        "\n",
        "Notice that we divided all the data into three subsets -- a training set `train`, a validation set `val`, and a test set `test`. The reason for this is to properly evaluate the accuracy of our classifier. During training, the optimizer uses the validation set to evaluate its internal performance, in order to determine the gradient without overfitting to the training set. The `test` set is always held out from the training algorithm, and is only used at the end to evaluate the final accuracy of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwMY_ZXYoax",
        "colab_type": "text"
      },
      "source": [
        "## Transfer learning by starting with existing network\n",
        "\n",
        "Now we can move on to the main strategy for training an image classifier on our small dataset: by starting with a larger and already trained network.\n",
        "\n",
        "To start, we will load the VGG16 from TorchVision, which was trained on ImageNet and the weights saved online. If this is your first time loading VGG16, you'll need to wait a bit for the weights to download from the web. Once the network is loaded, we can again inspect the layers with the `summary()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUDAbxiYoay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "outputId": "9ba556fc-a1d6-45f3-c19e-e440558ff42c"
      },
      "source": [
        "vgg = torchvision.models.vgg16(pretrained=False)\n",
        "vgg.to(torch.device(device))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLXTofcNYoa2",
        "colab_type": "text"
      },
      "source": [
        "Notice that VGG16 is _much_ bigger than the network we constructed earlier. It contains 13 convolutional layers and two fully connected layers at the end, and has over 138 million parameters, around 100 times as many parameters than the network we made above. Like our first network, the majority of the parameters are stored in the connections leading into the first fully-connected layer.\n",
        "\n",
        "VGG16 was made to solve ImageNet, and achieves a [8.8% top-5 error rate](https://github.com/jcjohnson/cnn-benchmarks), which means that 91.2% of test samples were classified correctly within the top 5 predictions for each image. It's top-1 accuracy--equivalent to the accuracy metric we've been using (that the top prediction is correct)--is 73%. This is especially impressive since there are not just 97, but 1000 classes, meaning that random guesses would get us only 0.1% accuracy.\n",
        "\n",
        "In order to use this network for our task, we \"remove\" the final classification layer, the 1000-neuron softmax layer at the end, which corresponds to ImageNet, and instead replace it with a new Linear layer for our dataset, which contains 10 layers in the case of STL_10.\n",
        "\n",
        "In terms of implementation, it's easier to simply create a copy of VGG from its input layer until the second to last layer, and then work with that, rather than modifying the VGG object directly. So technically we never \"remove\" anything, we just circumvent/ignore it. This can be done by adding a layer to our typical training loop. In this case we would add a softmax to the variable output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFL-fLitYoa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6f8c99f-8f07-48d6-b210-0e65cedd35d8"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "lastLayer = nn.Linear(1000,10).to(torch.device(device))\n",
        "\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(torch.device(device)), labels.to(torch.device(device))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = vgg(inputs)\n",
        "        outputs = lastLayer(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 25 == 24:    # print every 25 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 25))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    25] loss: 2.254\n",
            "[1,    50] loss: 2.128\n",
            "[1,    75] loss: 2.022\n",
            "[1,   100] loss: 2.031\n",
            "[1,   125] loss: 1.999\n",
            "[1,   150] loss: 1.956\n",
            "[1,   175] loss: 2.016\n",
            "[1,   200] loss: 1.958\n",
            "[1,   225] loss: 1.888\n",
            "[1,   250] loss: 1.872\n",
            "[1,   275] loss: 1.839\n",
            "[1,   300] loss: 1.876\n",
            "[1,   325] loss: 1.872\n",
            "[1,   350] loss: 1.831\n",
            "[1,   375] loss: 1.948\n",
            "[1,   400] loss: 1.842\n",
            "[2,    25] loss: 1.829\n",
            "[2,    50] loss: 1.765\n",
            "[2,    75] loss: 1.823\n",
            "[2,   100] loss: 1.789\n",
            "[2,   125] loss: 1.766\n",
            "[2,   150] loss: 1.804\n",
            "[2,   175] loss: 1.808\n",
            "[2,   200] loss: 1.789\n",
            "[2,   225] loss: 1.754\n",
            "[2,   250] loss: 1.798\n",
            "[2,   275] loss: 1.795\n",
            "[2,   300] loss: 1.755\n",
            "[2,   325] loss: 1.563\n",
            "[2,   350] loss: 1.692\n",
            "[2,   375] loss: 1.699\n",
            "[2,   400] loss: 1.622\n",
            "[3,    25] loss: 1.638\n",
            "[3,    50] loss: 1.701\n",
            "[3,    75] loss: 1.668\n",
            "[3,   100] loss: 1.626\n",
            "[3,   125] loss: 1.527\n",
            "[3,   150] loss: 1.609\n",
            "[3,   175] loss: 1.610\n",
            "[3,   200] loss: 1.707\n",
            "[3,   225] loss: 1.622\n",
            "[3,   250] loss: 1.590\n",
            "[3,   275] loss: 1.662\n",
            "[3,   300] loss: 1.572\n",
            "[3,   325] loss: 1.613\n",
            "[3,   350] loss: 1.641\n",
            "[3,   375] loss: 1.608\n",
            "[3,   400] loss: 1.594\n",
            "[4,    25] loss: 1.651\n",
            "[4,    50] loss: 1.590\n",
            "[4,    75] loss: 1.513\n",
            "[4,   100] loss: 1.452\n",
            "[4,   125] loss: 1.504\n",
            "[4,   150] loss: 1.499\n",
            "[4,   175] loss: 1.565\n",
            "[4,   200] loss: 1.493\n",
            "[4,   225] loss: 1.600\n",
            "[4,   250] loss: 1.567\n",
            "[4,   275] loss: 1.584\n",
            "[4,   300] loss: 1.543\n",
            "[4,   325] loss: 1.554\n",
            "[4,   350] loss: 1.544\n",
            "[4,   375] loss: 1.592\n",
            "[4,   400] loss: 1.622\n",
            "[5,    25] loss: 1.506\n",
            "[5,    50] loss: 1.540\n",
            "[5,    75] loss: 1.442\n",
            "[5,   100] loss: 1.429\n",
            "[5,   125] loss: 1.411\n",
            "[5,   150] loss: 1.491\n",
            "[5,   175] loss: 1.579\n",
            "[5,   200] loss: 1.382\n",
            "[5,   225] loss: 1.594\n",
            "[5,   250] loss: 1.505\n",
            "[5,   275] loss: 1.523\n",
            "[5,   300] loss: 1.426\n",
            "[5,   325] loss: 1.509\n",
            "[5,   350] loss: 1.453\n",
            "[5,   375] loss: 1.534\n",
            "[5,   400] loss: 1.516\n",
            "[6,    25] loss: 1.345\n",
            "[6,    50] loss: 1.322\n",
            "[6,    75] loss: 1.506\n",
            "[6,   100] loss: 1.419\n",
            "[6,   125] loss: 1.375\n",
            "[6,   150] loss: 1.433\n",
            "[6,   175] loss: 1.484\n",
            "[6,   200] loss: 1.439\n",
            "[6,   225] loss: 1.356\n",
            "[6,   250] loss: 1.463\n",
            "[6,   275] loss: 1.427\n",
            "[6,   300] loss: 1.386\n",
            "[6,   325] loss: 1.428\n",
            "[6,   350] loss: 1.296\n",
            "[6,   375] loss: 1.393\n",
            "[6,   400] loss: 1.542\n",
            "[7,    25] loss: 1.275\n",
            "[7,    50] loss: 1.262\n",
            "[7,    75] loss: 1.370\n",
            "[7,   100] loss: 1.302\n",
            "[7,   125] loss: 1.323\n",
            "[7,   150] loss: 1.344\n",
            "[7,   175] loss: 1.336\n",
            "[7,   200] loss: 1.356\n",
            "[7,   225] loss: 1.314\n",
            "[7,   250] loss: 1.318\n",
            "[7,   275] loss: 1.369\n",
            "[7,   300] loss: 1.407\n",
            "[7,   325] loss: 1.259\n",
            "[7,   350] loss: 1.283\n",
            "[7,   375] loss: 1.301\n",
            "[7,   400] loss: 1.271\n",
            "[8,    25] loss: 1.381\n",
            "[8,    50] loss: 1.149\n",
            "[8,    75] loss: 1.229\n",
            "[8,   100] loss: 1.219\n",
            "[8,   125] loss: 1.195\n",
            "[8,   150] loss: 1.208\n",
            "[8,   175] loss: 1.236\n",
            "[8,   200] loss: 1.233\n",
            "[8,   225] loss: 1.218\n",
            "[8,   250] loss: 1.340\n",
            "[8,   275] loss: 1.213\n",
            "[8,   300] loss: 1.383\n",
            "[8,   325] loss: 1.206\n",
            "[8,   350] loss: 1.263\n",
            "[8,   375] loss: 1.357\n",
            "[8,   400] loss: 1.241\n",
            "[9,    25] loss: 1.066\n",
            "[9,    50] loss: 1.184\n",
            "[9,    75] loss: 1.224\n",
            "[9,   100] loss: 1.195\n",
            "[9,   125] loss: 1.167\n",
            "[9,   150] loss: 1.205\n",
            "[9,   175] loss: 1.116\n",
            "[9,   200] loss: 1.098\n",
            "[9,   225] loss: 1.005\n",
            "[9,   250] loss: 1.459\n",
            "[9,   275] loss: 1.233\n",
            "[9,   300] loss: 1.225\n",
            "[9,   325] loss: 1.245\n",
            "[9,   350] loss: 1.140\n",
            "[9,   375] loss: 1.239\n",
            "[9,   400] loss: 1.145\n",
            "[10,    25] loss: 1.062\n",
            "[10,    50] loss: 0.971\n",
            "[10,    75] loss: 0.980\n",
            "[10,   100] loss: 1.121\n",
            "[10,   125] loss: 1.009\n",
            "[10,   150] loss: 1.039\n",
            "[10,   175] loss: 1.207\n",
            "[10,   200] loss: 1.009\n",
            "[10,   225] loss: 1.099\n",
            "[10,   250] loss: 1.002\n",
            "[10,   275] loss: 1.140\n",
            "[10,   300] loss: 1.063\n",
            "[10,   325] loss: 1.174\n",
            "[10,   350] loss: 1.189\n",
            "[10,   375] loss: 1.088\n",
            "[10,   400] loss: 1.172\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKR4V3SCc3du",
        "colab_type": "text"
      },
      "source": [
        "The model appears very functional after only 10 epochs. However, now we must freeze all weights to test it on the validation set. This test will demonstrate the effectiveness of our model while also making clear if the model is overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WLRdz1Ocp1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "110a3b22-aae3-4bb9-dbd7-81237a31c368"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(torch.device(device)), labels.to(torch.device(device))\n",
        "        outputs = vgg(inputs)\n",
        "        outputs = lastLayer(outputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 43 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ahKv8XYobY",
        "colab_type": "text"
      },
      "source": [
        "### Improving the results\n",
        "\n",
        "43% top-1 accuracy on 10 classes after 10 epochs, roughly evenly distributed, is a pretty good achievement. It is not quite as impressive as the original VGG16 which achieved 73% top-1 accuracy on 1000 classes. Nevertheless, it is much better than what we were able to achieve with our original network, and there is room for improvement. Some techniques which possibly could have improved our performance.\n",
        "\n",
        "- Using data augementation: augmentation refers to using various modifications of the original training data, in the form of distortions, rotations, rescalings, lighting changes, etc to increase the size of the training set and create more tolerance for such distortions.\n",
        "- Using a different optimizer, adding more regularization/dropout, and other hyperparameters.\n",
        "- Training for longer (of course)\n",
        "\n",
        "A more advanced example of transfer learning in Keras, involving augmentation for a small 2-class dataset, can be found in the [Keras blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
      ]
    }
  ]
}