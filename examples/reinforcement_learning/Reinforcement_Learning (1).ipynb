{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KChn1jNJdT-X"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "This tutorial is a modified version of pytorch's own Reinforcement Learning [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).\n",
        "\n",
        "Simply put, reinforcement learning is the cyclical process of Reward = State * Action. In psychological terms, the process is most akin to [positive reinforcement](https://en.wikipedia.org/wiki/Reinforcement#Positive_reinforcement), in which a subject is given a reward for completing a simple goal in order to increase the likelihood of doing said goal. \n",
        "\n",
        "Reinforcement learning is similar in that we intend to maximize a reward by promoting a given behavior, although we break down our ultimate goal into smaller actions, giving rewards along the way. Each action is made in response to a state, or the current conditions of the environment. \n",
        "\n",
        "But how do we know the current condtions of the environment? Or put another way, how can we make a statistical model to optimize a reward? We use a neural network. The beauty in this is it also allows the AI to act on inference given knowledge of similar states, as an neural network can approximate any statistical model, including the that of action, state, and reward.\n",
        "\n",
        "Let's see this is in action (ba-dum-tss)..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnXPLnannE6O"
      },
      "source": [
        "# Setting Up Packages\n",
        "\n",
        "We begin by collecting the ```pip``` package **gym**. Gym is a collection of environments made by OpenAI that can be used to train our neural network. In this case, we will build our model based on their atari clones. Specifically, [Atari Breakout](https://gym.openai.com/envs/Breakout-v0/).\n",
        "\n",
        "If not in colab, run ```pip install gym```.\n",
        "\n",
        "Next we import our libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tensmrXXrEBI"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = gym.make('Breakout-v0').unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP7B_P3krslC"
      },
      "source": [
        "# Replay Memory\n",
        "\n",
        "Replay memory is our means of storing information about prior states and what was done to maximize rewards in a given state. This gives us the ability to compute inference of the current state faster, as well as have a successful model faster.\n",
        "For this we need two objects:\n",
        "\n",
        "\n",
        "*   **Transition**: This is a named tuple that demonstrates to the AI what the concequences will be given the state and action.\n",
        "*   **Replay Memory**: A means of storing a sequence of transitions and sampling them at random for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQcMG6ftttNf"
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcFWjkM2wh3y"
      },
      "source": [
        "# Q-Network\n",
        "\n",
        "This is the main neural network we will be running. The most critical idea to take away from it is the number of outputs must equal the total number of possible number of moves for a game controller. This way, we can predict the action (or move) that gives the highest reward. In this case, our moves include left and right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANEXHr1-yej1"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        # Input dimension of 1 because color doesn't really matter\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
        "        self.fc1 = nn.Linear(1472 * 17, 256)\n",
        "        self.head = nn.Linear(256, 2)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = (x.view(-1,1472 * 17))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.head(x.view(x.size(0), -1))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7eoe2tk9Mlv"
      },
      "source": [
        "# Scene Extraction\n",
        "\n",
        "Below is the code we use to grab the screen for input into the Q-Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSYWHZsM9ZQK",
        "outputId": "db884a2e-6e74-465e-8af2-020fd3f503eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Grayscale(),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def get_screen():\n",
        "\n",
        "    screen = env.render(mode=\"rgb_array\").transpose((2, 0, 1))\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "# Extracted Scene\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(1).squeeze(0).numpy(), cmap=\"gray\")\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6JLBIJaEk5j"
      },
      "source": [
        "# Utilities\n",
        "\n",
        "To train our model, we will rely on both past experience and randomness. The element of randomness allows us to find new methods not previously discovered. This can be useful, but eventually, we want the element of randomness to decay.\n",
        "\n",
        "Our measurement of success is the length of time our player stays alive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yf7bwlnHk5s"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            \n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcbcb9TOIAfr"
      },
      "source": [
        "# Training\n",
        "\n",
        "At last, we train and optimize our model. For those curious, we use a different loss function known as Huber Loss as it reduces noise of outliers, behaving as different losses depending on the situation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88J-dJPsIA9P"
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(0, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shn2olo_JlGd"
      },
      "source": [
        "Finally, we enter the main training loop. We run through the episodes, and display the final one. Cograts on making it this far!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qElvKYlZJSOY",
        "outputId": "65212292-c088-4c1b-b400-18d047f85c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "# Similar to epochs\n",
        "num_episodes = 15\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "    print(i_episode)\n",
        "print('Complete')\n",
        "\n",
        "# Hey Gene, not sure what to do here because I couldn't find a way to render \n",
        "# Gym without making the rest of the notebook look sloppy. Let me know what\n",
        "# you think we should do here.\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "Complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NoSuchDisplayException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e1f37dbcc062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mioff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}